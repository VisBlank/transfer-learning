{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment Setting:\n",
    "1. Train with a large neural net with 2 hidden layers of 1200 rectified linear hidden units.\n",
    "2. Distill the knowledge from neural net above to a two-layer distilled net with 800 rectified linear hidden units(at the temperature of 20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting teacher.py\n"
     ]
    }
   ],
   "source": [
    "# %%writefile teacher.py\n",
    "# %load teacher.py\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "model = None\n",
    "optimizer = None\n",
    "epochs = 20\n",
    "batch_size = 128\n",
    "lr = 1e-4\n",
    "\n",
    "class Teacher(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Teacher, self).__init__()\n",
    "        self.n_inputs = 28 * 28\n",
    "        self.n_layer_1 = 1200\n",
    "        self.n_layer_2 = 1200\n",
    "        self.n_classes = 10\n",
    "        self.drop = nn.Dropout(0.5)\n",
    "        self.affine1 = nn.Linear(self.n_inputs, self.n_layer_1)\n",
    "        self.affine2 = nn.Linear(self.n_layer_1, self.n_layer_2)\n",
    "        self.affine3 = nn.Linear(self.n_layer_2, self.n_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.n_inputs)\n",
    "        out1 = self.drop(F.relu(self.affine1(x)))\n",
    "        out2 = self.drop(F.relu(self.affine2(out1)))\n",
    "        out3 = self.affine3(out2)\n",
    "        return out3\n",
    "    \n",
    "def train():\n",
    "    model.train()\n",
    "    for epoch in xrange(epochs):\n",
    "        avg_loss = 0\n",
    "        n_batches = len(train_loader)\n",
    "        for batch_idx, (data, label) in enumerate(train_loader):\n",
    "            data, label = data.cuda(), label.cuda()\n",
    "            data, label = Variable(data), Variable(label)\n",
    "            optimizer.zero_grad()\n",
    "            output = F.log_softmax(model(data))\n",
    "            loss = F.nll_loss(output, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.data[0]\n",
    "        avg_loss /= n_batches\n",
    "        print(avg_loss)\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data, label in test_loader:\n",
    "        data, label = data.cuda(), label.cuda()\n",
    "        data, label = Variable(data, volatile=True), Variable(label)\n",
    "        output = F.log_softmax(model(data))\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct += pred.eq(label.data.view_as(pred)).cpu().sum()\n",
    "    \n",
    "    print(100. * correct / len(test_loader.dataset))\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    model = Teacher()\n",
    "    model.cuda()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n",
    "    train()\n",
    "    with open('teacher.params', 'wb') as f:\n",
    "        torch.save(model, f)\n",
    "    test()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting student.py\n"
     ]
    }
   ],
   "source": [
    "# %%writefile student.py\n",
    "# %load student.py\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class Student(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Student, self).__init__()\n",
    "        self.n_inputs = 28 * 28\n",
    "        self.n_layer_1 = 800\n",
    "        self.n_layer_2 = 800\n",
    "        self.n_classes = 10\n",
    "        self.drop = nn.Dropout(0.5)\n",
    "        self.affine1 = nn.Linear(self.n_inputs, self.n_layer_1)\n",
    "        self.affine2 = nn.Linear(self.n_layer_1, self.n_layer_2)\n",
    "        self.affine3 = nn.Linear(self.n_layer_2, self.n_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.n_inputs)\n",
    "        out1 = self.drop(F.relu(self.affine1(x)))\n",
    "        out2 = self.drop(F.relu(self.affine2(out1)))\n",
    "        out3 = self.affine3(out2)\n",
    "        return out3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher (\n",
      "  (drop): Dropout (p = 0.5)\n",
      "  (affine1): Linear (784 -> 1200)\n",
      "  (affine2): Linear (1200 -> 1200)\n",
      "  (affine3): Linear (1200 -> 10)\n",
      ")\n",
      "0.316319770031\n",
      "0.315715478534\n",
      "0.315667766149\n",
      "0.315655072624\n",
      "0.315634672758\n",
      "95.51\n"
     ]
    }
   ],
   "source": [
    "# %%writefile teacher-student.py\n",
    "# %load teacher-student.py\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from student import *\n",
    "from teacher import *\n",
    "\n",
    "teacher = None\n",
    "student = None\n",
    "epochs = 5\n",
    "batch_size = 128\n",
    "lr = 1e-3\n",
    "\n",
    "optimizer = None\n",
    "\n",
    "def train():\n",
    "    T = 20\n",
    "    student.train()\n",
    "    for epoch in xrange(epochs):\n",
    "        avg_loss = 0\n",
    "        n_batches = len(transfer_loader)\n",
    "        for data, label in transfer_loader:\n",
    "            data, label = data.cuda(), label.cuda()\n",
    "            data, label = Variable(data), Variable(label)\n",
    "            optimizer.zero_grad()\n",
    "            output_teacher = F.softmax(teacher(data) / T)\n",
    "            output_student = F.softmax(student(data) / T)\n",
    "            loss = F.binary_cross_entropy(output_student, output_teacher)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.data[0]\n",
    "        avg_loss /= n_batches\n",
    "        print(avg_loss)\n",
    "        \n",
    "    \n",
    "def test():\n",
    "    T = 1\n",
    "    student.eval()\n",
    "    correct = 0\n",
    "    for data, label in test_loader:\n",
    "        data, label = data.cuda(), label.cuda()\n",
    "        data, label = Variable(data, volatile=True), Variable(label)\n",
    "        output = F.log_softmax(student(data) / T)\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct += pred.eq(label.data.view_as(pred)).cpu().sum()\n",
    "    \n",
    "    print(100. * correct / len(test_loader.dataset))\n",
    "\n",
    "student = Student()\n",
    "student.cuda()\n",
    "with open('teacher.params', 'rb') as f:\n",
    "    teacher = torch.load(f)\n",
    "\n",
    "optimizer = optim.Adam(student.parameters(), lr=lr)\n",
    "\n",
    "train_set = datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ]))\n",
    "\n",
    "transfer_data, transfer_labels = [], []\n",
    "for data, label in train_set:\n",
    "    if label != 3:\n",
    "        transfer_data.append(data.tolist())\n",
    "        transfer_labels.append(label)\n",
    "        \n",
    "transfer_data, transfer_labels = torch.Tensor(transfer_data), torch.Tensor(transfer_labels)\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "transfer_loader = torch.utils.data.DataLoader(\n",
    "        torch.utils.data.TensorDataset(transfer_data, transfer_labels),\n",
    "        batch_size=batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.1307,), (0.3081,))\n",
    "                ])),\n",
    "        batch_size=batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "print(teacher)\n",
    "\n",
    "teacher.eval()\n",
    "train()\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
